# airavata-quantization-fastapi
# 🚀 Quantized Deployment of Ai4bharat/Airavata LLM with FastAPI

## 🔧 Project Overview
This project demonstrates the quantization of the Ai4bharat/Airavata LLM for efficient inference on CPU and GPU using [quantization method, e.g., `GPTQ`, `BitsandBytes`]. The quantized model is integrated with a FastAPI backend for serving predictions with minimal latency.

## ⚙️ Features
- ✅ Quantized Airavata LLM (CPU + GPU support)
- ✅ FastAPI Backend for text generation
- ✅ Latency & Throughput Benchmarking
- ✅ Runs on Google Colab / Local / Cloud
- ✅ Clean code structure with modular design
  
## 🚀 Demo Output

Below is a sample response from the quantized Airavata model using the FastAPI backend:

![API Output](screenshot_api_output.jpg)

  
## Author
Aasika ES
M.Sc. Data Science Graduate
📧 [aasikasivaji@gmail.com]
