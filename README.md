# airavata-quantization-fastapi
# ğŸš€ Quantized Deployment of Ai4bharat/Airavata LLM with FastAPI

## ğŸ”§ Project Overview
This project demonstrates the quantization of the Ai4bharat/Airavata LLM for efficient inference on CPU and GPU using [quantization method, e.g., `GPTQ`, `BitsandBytes`]. The quantized model is integrated with a FastAPI backend for serving predictions with minimal latency.

## âš™ï¸ Features
- âœ… Quantized Airavata LLM (CPU + GPU support)
- âœ… FastAPI Backend for text generation
- âœ… Latency & Throughput Benchmarking
- âœ… Runs on Google Colab / Local / Cloud
- âœ… Clean code structure with modular design
  
## ğŸš€ Demo Output

Below is a sample response from the quantized Airavata model using the FastAPI backend:

![API Output](screenshot_api_output.jpg)

  
## Author
Aasika ES
M.Sc. Data Science Graduate
ğŸ“§ [aasikasivaji@gmail.com]
